---
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: [default, fonts.css, animate.css]
    ratio: '16:9'
    includes:
      in_header: fa.html
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(infer)
library(gt)
library(nur1027functions)
library(moderndive)
```

class: center, middle, hide-count

# Introduction

???

I think for you to properly interpret regression coefficients, you need to have a basic understanding of what linear regression actually is. To demonstrate, let's consider the example from my previous research that we looked at previously to learn about validity.

---
class: top, left, hide-count


```{r echo=FALSE, fig.align="left", fig.retina=8, fig.height=4}
plot <- plot_qol(x=PHQ9, y=GH) +
  labs(x = "Depression (PHQ-9)", y= "Quality of life (SF-36)")
plot
```
--
.left-column[
Regression quantifies the relationship between an *outcome* and *predictor* variable by finding the line of best fit


Outcome is quality of life and predictor is depression
]

???

This plot shows quality of life on the vertical axis and depression scores on the horizintal axis of a sample of heart transplant recipients. Quality of life was measured with the SF thirty six on a scale from zero to one hundred with higher scores indicating better quality of life. Depression was measured with the PHQ-nine on a scale from zero to twenty five with higher scores indicating worse depression. What we can see by just visually inspecting the data, is that there may be a negative correlation between these two measures because the clustering of scores at the top left make it seem like patients who had higher quality of life may have had lower depression scores. Linear regression provides us with insights into the relationship between an outcome and predictor variable by finding the line of best fit between all these points. In our case, the outcome is quality of life and the predictor is depression.

---
class: top, left, hide-count

.left-column[
```{r echo=FALSE, fig.align="left", fig.retina=8, fig.height=4}
plot+
    theme_minimal()+
    scale_y_continuous(limits = c(0,100))+
    scale_x_continuous(limits = c(0,25))+
  geom_smooth(method = "lm", se = FALSE)+
  annotate(
    geom = "curve", xend = 0, yend = 67, x = 7.5, y = 3, 
    curvature = -.3, arrow = arrow(length = unit(4, "mm"))
  ) +
  annotate(geom = "text", x = 8, y = 3, label = "Intercept (67)", hjust = "left",
           size = 5)
```

Regression line characterised by the **intercept** and **slope** coefficients

Intercept is the point at which the regression line crosses the vertical axis

Slope coefficient tells us the magnitude of change that we should expect to see in the outcome variable for a unit increase in the predictor variable

]
???

This is what the regression line looks like. We can see that it indeed does kind of slope down from the top left of the plot, just as we suspected it might do. If we calculate the regression coefficients, we will be able to quantify this linear relationship between the outcome and predictor variable. The coefficients we calculate will be one coefficient for the intercept and a coefficient for the slope. The intercept is the point at which the regression line crosses the vertical axis. In this case, it is 67.  The slope coefficient will tell us the magnitude of change that we should expect to see in the outcome variable for each unit increase in the predictor variable. Let's take a closer look at the slope coefficient.

---

class: top, left, hide-count

```{r echo=FALSE, fig.align="left", fig.retina=8, fig.height=4}
plot+
    theme_minimal()+
    scale_y_continuous(limits = c(0,100))+
    scale_x_continuous(limits = c(0,25), breaks = c(0,5,10,15,17.5, 20, 25))+
  geom_smooth(method = "lm", se = FALSE)+    coord_cartesian(ylim=c(40, 50))

```
???

In our case, the regression line is not very steep so we need to zoom in the plot to be able to appreciate how the slope coefficient relates to the regression line. This is the same plot as before except just showing the vertical axis from forty to fifty points instead of zero to one hundred so the line just gives the impression that it is steeper.
---

class: top, left, hide-count

```{r echo=FALSE, fig.align="left", fig.retina=8, fig.height=4}
plot+
    theme_minimal()+
    scale_y_continuous(limits = c(0,100))+
    scale_x_continuous(limits = c(0,25), breaks = c(0,5,10,15,17.5, 20, 25))+
  geom_smooth(method = "lm", se = FALSE)+
    coord_cartesian(ylim=c(40, 50))+
  annotate(
    geom = "segment", xend = 18.5, yend = 42.5, x = 17.5, y = 42.5, color = "red"
  )
```
???

To demonstrate how the slope coefficient is calculated, we can take the point in the regression line at the coordinates where quality of life equals forty two point five and depression equals seventeen point five and then draw a straight horizontal line one point along to a depression score of eighteen point five.

---

class: top, left, hide-count

```{r echo=FALSE, fig.align="left", fig.retina=8, fig.height=4}
plot+
    theme_minimal()+
    scale_y_continuous(limits = c(0,100))+
    scale_x_continuous(limits = c(0,25), breaks = c(0,5,10,15,17.5, 20, 25))+
  geom_smooth(method = "lm", se = FALSE)+
  annotate(
    geom = "segment", xend = 18.5, yend = 42.5, x = 17.5, y = 42.5, color = "red"
  ) +
  annotate(
    geom = "segment", xend = 18.5, yend = 42.5-1.42, x = 18.5, y = 42.5, color = "red"
  ) +
  coord_cartesian(ylim=c(40, 50))+
  annotate(geom = "text", x = 19, y = 42.5-(1.42/2), label = "1.4 points lower", hjust = "left",
           size = 4)
```
.left-column[

For each unit INCREASE in depression, there was an associated DECREASE of, on average, 1.42 units of quality of life

]

???

Then, if we draw a straight vertical line back to meet the regression line again and calculate how many quality of points down we had to go, that will give us our slope coefficient. In this case, because we had to go vertically downwards the slope coefficient is negative one point four. And the interpretation, is that for each unit increase in depression, there was an associated decrease of one point four units of quality of life.

---
class: top, left, hide-count

# $\beta$ coefficients

```{r echo=FALSE, fig.align="left"}
model <- lm(GH ~ PHQ9, data = data)
moderndive::get_regression_table(model)%>% 
  gt() %>% 
  cols_label(
    estimate = html("&beta;")
  )
```
--
.left-column[
Null hypothesis for linear regression is that there is **no relationship** between the predictor and outcome variables

$$ H_0: \beta=0 $$
$$ H_1: \betaâ‰ 0$$

]

???

Somthing important to note is that instead of showing a scatterplot with the fitted regression line, many research papers will report results of linear regression in a table like this. First of all, you'll notice that the coefficients are commonly termed beta coefficients with the greek symbol used for notation. The first row of the table shows the coefficient for the intercept and the second has the slope coefficient for the depression variable along with it's associated p value and confidence intervals. And this brings me to my next point. When I've previously talked about p-values, it has been associated with testing the null hypothesis that there was no difference in mean scores between two groups. In the case of linear regression, the p-value is instead testing the null hypothesis that there is no relationship between the predictor and the outcome variable. Or in other words, the null hypothesis is that the regression coefficient is zero and the alternative hypothesis is that the regression coefficient is not zero. 

---
class: center, middle, hide-count

# Up next

???

In the next section, I want to test your knowledge a bit and see if you can properly interpret what the 95% confidence intervals presented in this table means.